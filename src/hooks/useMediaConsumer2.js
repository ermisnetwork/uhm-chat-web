import { useCallback, useRef } from 'react';

export const useMediaConsumer2 = (channelId, videoRef, nodeRef) => {
  const videoDecoderRef = useRef(null);
  const videoWriterRef = useRef(null);
  const audioDecoderRef = useRef(null);
  const combinedStreamRef = useRef(null);
  const isWaitingForKeyFrame = useRef(true);
  const audioContextRef = useRef(null);
  const mediaDestinationRef = useRef(null);
  const nextStartTimeRef = useRef(0);

  const initAudioContext = useCallback(async () => {
    if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });

      mediaDestinationRef.current = audioContextRef.current.createMediaStreamDestination();

      // Resume AudioContext náº¿u bá»‹ suspended (cáº§n user interaction)
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }

      nextStartTimeRef.current = audioContextRef.current.currentTime;
      // firstPacketRef.current = true;
    }
  }, []);

  const playDecodedAudio = useCallback(async audioData => {
    try {
      const { numberOfChannels, numberOfFrames, sampleRate, format } = audioData;
      const audioContext = audioContextRef.current;
      if (!audioContext) return;

      // Táº¡o AudioBuffer
      const audioBuffer = audioContext.createBuffer(numberOfChannels, numberOfFrames, sampleRate);

      // âœ… Táº¡o buffer Ä‘á»ƒ copy dá»¯ liá»‡u
      const dataSize = numberOfFrames * numberOfChannels;
      const tempBuffer = new Float32Array(dataSize);

      // âœ… Copy dá»¯ liá»‡u tá»« AudioData vá»›i format f32-planar
      audioData.copyTo(tempBuffer, {
        planeIndex: 0,
        format: 'f32-planar',
      });

      // Copy dá»¯ liá»‡u vÃ o tá»«ng kÃªnh
      for (let ch = 0; ch < numberOfChannels; ch++) {
        const channelData = audioBuffer.getChannelData(ch);
        const offset = ch * numberOfFrames; // offset cho tá»«ng kÃªnh trong planar format

        for (let i = 0; i < numberOfFrames; i++) {
          channelData[i] = tempBuffer[offset + i];
        }
      }

      // Táº¡o nguá»“n phÃ¡t Ã¢m thanh
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;

      const gainNode = audioContext.createGain();
      gainNode.gain.value = 1.0;
      source.connect(gainNode).connect(mediaDestinationRef.current);

      // Äáº£m báº£o phÃ¡t liÃªn tá»¥c
      if (nextStartTimeRef.current < audioContext.currentTime) {
        nextStartTimeRef.current = audioContext.currentTime;
      }

      source.start(nextStartTimeRef.current);
      nextStartTimeRef.current += numberOfFrames / sampleRate;

      // Giáº£i phÃ³ng bá»™ nhá»›
      if (audioData && typeof audioData.close === 'function') {
        audioData.close();
      }
    } catch (err) {
      console.error('Error in playDecodedAudio:', err);
      if (audioData && typeof audioData.close === 'function') {
        audioData.close();
      }
    }
  }, []);

  const initDecoders = useCallback(() => {
    // ÄÃ³ng decoders cÅ© trÆ°á»›c
    if (videoDecoderRef.current && videoDecoderRef.current.state !== 'closed') {
      videoDecoderRef.current.close();
    }
    if (audioDecoderRef.current && audioDecoderRef.current.state !== 'closed') {
      audioDecoderRef.current.close();
    }

    // Táº¡o video track
    const videoTrackGenerator = new MediaStreamTrackGenerator({ kind: 'video' });
    videoWriterRef.current = videoTrackGenerator.writable.getWriter();

    // Táº¡o AudioContext + MediaStreamDestination
    initAudioContext();

    // Láº¥y audio track tá»« AudioContext
    const audioTrack = mediaDestinationRef.current.stream.getAudioTracks()[0];

    const combinedStream = new MediaStream([videoTrackGenerator, audioTrack]);
    combinedStreamRef.current = combinedStream;

    // GÃ¡n cho videoRef
    if (videoRef.current) {
      videoRef.current.srcObject = combinedStream;
      videoRef.current.muted = false;
    }

    isWaitingForKeyFrame.current = true;

    // Init video decoder
    const videoDecoder = new VideoDecoder({
      output: async frame => {
        try {
          await videoWriterRef.current.write(frame);
        } catch (err) {
          frame.close();
        }
      },
      error: err => {
        console.error('VideoDecoder error:', err);
        isWaitingForKeyFrame.current = true;
      },
    });

    const desc =
      'AQFgAAAAsAAAAAAAXfAA/P34+AAADwOgAAEAGEABDAH//wFgAAADALAAAAMAAAMAXRsCQKEAAQAtQgEBAWAAAAMAsAAAAwAAAwBdoAKAgC0WIG7kUjLn4T0L6hvVKagwCDAfwgEEogABAAdEAcBy8Fsk';
    // const desc =
    //   'ASFgAAADAAADAAADAPAA/P34+AAADwOgAAEAGEABDAH//yFgAAADAAADAAADAAADAHgsCaEAAQAjQgEBIWAAAAMAAAMAAAMAAAMAeKACgIAtFotJcEu5qAgICASiAAEACEQBwGYrA7GQ';
    const videoDescriptionBytes = Uint8Array.from(atob(desc), c => c.charCodeAt(0)).buffer;

    videoDecoder.configure({
      codec: 'hev1.1.6.L93.B0',
      codedWidth: 1280,
      codedHeight: 720,
      description: videoDescriptionBytes,
    });

    videoDecoderRef.current = videoDecoder;

    // Init audio decoder
    const audioDecoder = new AudioDecoder({
      output: async audioData => {
        // console.log('--audioData--', audioData);
        playDecodedAudio(audioData);
      },
      error: err => {
        console.error('AudioDecoder error:', err);
      },
    });

    audioDecoder.configure({
      codec: 'opus',
      sampleRate: 48000,
      numberOfChannels: 1,
    });

    audioDecoderRef.current = audioDecoder;
  }, [videoRef]);

  const connectConsumer = useCallback(async () => {
    initDecoders();

    await nodeRef.current.acceptConnection();
    console.log('-----acceptConnection----');

    await nodeRef.current.acceptBidiStream();

    console.log('-----acceptBidiStream----');

    while (true) {
      try {
        const data = await nodeRef.current.asyncRecv();

        // âœ… Chuyá»ƒn Uint8Array thÃ nh ArrayBuffer
        const buffer = data.buffer.slice(data.byteOffset, data.byteOffset + data.byteLength);
        console.log('-----buffer.byteLength----', buffer.byteLength);

        if (buffer.byteLength < 5) {
          console.warn('âš ï¸ Invalid packet: too small', buffer.byteLength);
          return;
        }

        const view = new DataView(buffer);
        const timestamp = view.getUint32(0, false);
        const frameType = view.getUint8(4);
        console.log('---frameType--', frameType);

        const payload = buffer.slice(5);

        // ðŸŽ¥ Video frames (key=0, delta=1)
        if (frameType === 0 || frameType === 1) {
          if (!videoDecoderRef.current || videoDecoderRef.current.state !== 'configured') {
            console.warn('âš ï¸ VideoDecoder not ready, state:', videoDecoderRef.current?.state);
            return;
          }

          const isKeyFrame = frameType === 0;

          // âœ… Chá»‰ decode khi Ä‘Ã£ cÃ³ key frame hoáº·c frame hiá»‡n táº¡i lÃ  key frame
          if (isWaitingForKeyFrame.current) {
            if (!isKeyFrame) {
              console.log('ðŸ”„ Skipping delta frame, still waiting for key frame');
              return;
            }
            console.log('ðŸŽ¯ First key frame received after configure');
          }

          try {
            const chunk = new EncodedVideoChunk({
              type: frameType === 0 ? 'key' : 'delta',
              timestamp: timestamp * 1000,
              data: payload,
            });

            videoDecoderRef.current.decode(chunk);

            // âœ… Náº¿u decode key frame thÃ nh cÃ´ng, cho phÃ©p decode delta frames
            if (isKeyFrame) {
              isWaitingForKeyFrame.current = false;
              console.log('âœ… Key frame decoded successfully, now accepting delta frames');
            }
          } catch (e) {
            console.error('âŒ Video decode error:', e);

            if (e.name === 'InvalidStateError' || e.name === 'DataError') {
              if (videoDecoderRef.current && videoDecoderRef.current.state !== 'closed') {
                videoDecoderRef.current.close();
              }
              videoDecoderRef.current = null;
              isWaitingForKeyFrame.current = true;
            }
          }
        }
        // ðŸ”Š Audio frames (audio=2)
        else if (frameType === 2) {
          if (!audioDecoderRef.current || audioDecoderRef.current.state !== 'configured') {
            console.warn('âš ï¸ AudioDecoder not ready, state:', audioDecoderRef.current?.state);
            return;
          }
          try {
            const chunk = new EncodedAudioChunk({
              type: 'key',
              timestamp: timestamp * 1000,
              data: payload,
            });
            audioDecoderRef.current.decode(chunk);
          } catch (e) {
            console.error('âŒ Audio decode error:', e);

            if (e.name === 'InvalidStateError' || e.name === 'DataError') {
              console.log('ðŸ”„ Reinitializing audio decoder due to error...');

              // ÄÃ³ng decoder hiá»‡n táº¡i
              if (audioDecoderRef.current && audioDecoderRef.current.state !== 'closed') {
                audioDecoderRef.current.close();
              }
              audioDecoderRef.current = null;
            }
          }
        } else {
          console.warn('âš ï¸ Unknown frame type:', frameType);
        }
      } catch (error) {}
    }
  }, [channelId, initDecoders]);

  return { connectConsumer };
};
