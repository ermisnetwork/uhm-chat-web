import { useCallback, useEffect, useRef } from 'react';

// --- C·∫§U H√åNH ƒê·ªíNG B·ªò (SYNC CONFIG) ---
// Gi√° tr·ªã n√†y (ms) d√πng ƒë·ªÉ k√¨m h√£m Video l·∫°i, ch·ªù Audio ph√°t ra loa.
// - N·∫øu Video v·∫´n ch·∫°y NHANH h∆°n ti·∫øng: TƒÇNG s·ªë n√†y l√™n (v√≠ d·ª•: 150, 200).
// - N·∫øu Video b·ªã CH·∫¨M h∆°n ti·∫øng: GI·∫¢M s·ªë n√†y xu·ªëng (v√≠ d·ª•: 50, 0).
const MANUAL_VIDEO_DELAY_MS = 200;

export const useMediaDecoderSync = (remoteVideoRef, nodeRef) => {
  // Refs cho WebCodecs
  const videoDecoderRef = useRef(null);
  const audioDecoderRef = useRef(null);
  const videoWriterRef = useRef(null);

  // Refs cho Audio Context
  const audioContextRef = useRef(null);
  const mediaDestinationRef = useRef(null);
  const nextAudioStartTimeRef = useRef(0);

  // Refs cho qu·∫£n l√Ω Buffer v√† Sync
  const videoBufferRef = useRef([]); // H√†ng ƒë·ª£i Video Frames
  const isWaitingForKeyFrame = useRef(true);
  const syncStateRef = useRef({
    firstAudioTimestamp: null, // Timestamp c·ªßa g√≥i audio ƒë·∫ßu ti√™n (ms)
    audioContextStartTime: null, // Th·ªùi ƒëi·ªÉm AudioContext (s) t∆∞∆°ng ·ª©ng
    isReady: false,
  });
  const renderLoopIdRef = useRef(null);

  // --- 1. KH·ªûI T·∫†O AUDIO CONTEXT ---
  const initAudioContext = useCallback(async () => {
    if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
      const AudioContextClass = window.AudioContext || window.webkitAudioContext;
      audioContextRef.current = new AudioContextClass({ sampleRate: 48000, latencyHint: 'interactive' });

      mediaDestinationRef.current = audioContextRef.current.createMediaStreamDestination();

      // Resume n·∫øu tr√¨nh duy·ªát ch·∫∑n Autoplay
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }

      nextAudioStartTimeRef.current = audioContextRef.current.currentTime;
    }
  }, []);

  // --- 2. PLAY AUDIO (MASTER CLOCK) ---
  const playDecodedAudio = useCallback(async (audioData, timestampMs) => {
    try {
      const ctx = audioContextRef.current;
      if (!ctx) return;

      // Thi·∫øt l·∫≠p m·ªëc th·ªùi gian ƒë·ªìng b·ªô (Sync Anchor) khi nh·∫≠n g√≥i Audio ƒë·∫ßu ti√™n
      if (syncStateRef.current.firstAudioTimestamp === null) {
        syncStateRef.current.firstAudioTimestamp = timestampMs;
        syncStateRef.current.audioContextStartTime = nextAudioStartTimeRef.current;
        syncStateRef.current.isReady = true;
        console.log(
          `‚ö° Sync Anchor Set: AudioTS=${timestampMs}ms mapped to CtxTime=${nextAudioStartTimeRef.current.toFixed(3)}s`,
        );
      }

      // Convert AudioData th√†nh AudioBuffer
      const { numberOfChannels, numberOfFrames, sampleRate } = audioData;
      const audioBuffer = ctx.createBuffer(numberOfChannels, numberOfFrames, sampleRate);

      // Copy d·ªØ li·ªáu planar v√†o buffer
      const size = numberOfFrames * numberOfChannels;
      const tempBuffer = new Float32Array(size);
      audioData.copyTo(tempBuffer, { planeIndex: 0, format: 'f32-planar' });

      for (let ch = 0; ch < numberOfChannels; ch++) {
        const channelData = audioBuffer.getChannelData(ch);
        const offset = ch * numberOfFrames;
        for (let i = 0; i < numberOfFrames; i++) {
          channelData[i] = tempBuffer[offset + i];
        }
      }

      // T·∫°o Source v√† n·ªëi v√†o Destination
      const source = ctx.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(mediaDestinationRef.current);

      // Logic x·∫øp h√†ng ƒë·ª£i √¢m thanh (Queue) ƒë·ªÉ kh√¥ng b·ªã gap
      if (nextAudioStartTimeRef.current < ctx.currentTime) {
        nextAudioStartTimeRef.current = ctx.currentTime;
      }

      source.start(nextAudioStartTimeRef.current);
      nextAudioStartTimeRef.current += numberOfFrames / sampleRate;

      audioData.close();
    } catch (err) {
      console.error('‚ùå Audio play error:', err);
      audioData.close();
    }
  }, []);

  // --- 3. RENDER LOOP (SYNC VIDEO TO AUDIO) ---
  const startRenderLoop = useCallback(() => {
    const loop = async () => {
      // N·∫øu ch∆∞a s·∫µn s√†ng, ti·∫øp t·ª•c ƒë·ª£i
      if (!videoWriterRef.current || !audioContextRef.current || !syncStateRef.current.isReady) {
        renderLoopIdRef.current = requestAnimationFrame(loop);
        return;
      }

      const ctx = audioContextRef.current;
      const state = syncStateRef.current;

      // --- T√çNH TO√ÅN TH·ªúI GIAN M·ª§C TI√äU ---

      // L·∫•y ƒë·ªô tr·ªÖ ph·∫ßn c·ª©ng (n·∫øu tr√¨nh duy·ªát h·ªó tr·ª£)
      const hardwareLatency = (ctx.outputLatency || 0) + (ctx.baseLatency || 0);

      // T·ªïng ƒë·ªô tr·ªÖ c·∫ßn b√π = Tr·ªÖ ph·∫ßn c·ª©ng + Tr·ªÖ th·ªß c√¥ng (MANUAL_VIDEO_DELAY_MS)
      const totalCompensationMs = hardwareLatency * 1000 + MANUAL_VIDEO_DELAY_MS;

      // Th·ªùi gian ƒë√£ tr√¥i qua k·ªÉ t·ª´ m·ªëc Anchor
      const timeElapsed = ctx.currentTime - state.audioContextStartTime;

      // Th·ªùi ƒëi·ªÉm Video c·∫ßn hi·ªÉn th·ªã = (M·ªëc Audio g·ªëc + Th·ªùi gian tr√¥i qua) - B√π tr·ªÖ
      const currentVideoTargetTime = state.firstAudioTimestamp + timeElapsed * 1000 - totalCompensationMs;

      // --- DUY·ªÜT VIDEO BUFFER ---
      while (videoBufferRef.current.length > 0) {
        const frame = videoBufferRef.current[0];
        const frameTimestamp = frame.timestamp / 1000; // convert microseconds -> milliseconds

        // CASE A: Frame qu√° c≈© (Late frame) -> Drop ƒë·ªÉ ƒëu·ªïi k·ªãp
        // N·∫øu frame ch·∫≠m h∆°n target qu√° 40ms
        if (frameTimestamp < currentVideoTargetTime - 40) {
          // console.warn("‚è© Dropping late frame", frameTimestamp);
          frame.close();
          videoBufferRef.current.shift();
          continue;
        }

        // CASE B: Frame ·ªü t∆∞∆°ng lai (Early frame) -> Ch·ªù
        // N·∫øu frame l·ªõn h∆°n target + 15ms
        if (frameTimestamp > currentVideoTargetTime + 15) {
          break; // Tho√°t v√≤ng l·∫∑p, ch·ªù l·∫ßn render sau
        }

        // CASE C: Frame ƒë√∫ng th·ªùi ƒëi·ªÉm (On time) -> V·∫Ω
        try {
          await videoWriterRef.current.write(frame);
        } catch (e) {
          // Ignore write errors (stream closed, etc.)
          frame.close();
        }
        videoBufferRef.current.shift();
        break; // V·∫Ω xong 1 frame th√¨ ngh·ªâ, ch·ªù next animation frame
      }

      renderLoopIdRef.current = requestAnimationFrame(loop);
    };

    renderLoopIdRef.current = requestAnimationFrame(loop);
  }, []);

  // --- 4. KH·ªûI T·∫†O DECODERS & STREAMS ---
  const initDecoders = useCallback(() => {
    // Cleanup c≈©
    if (videoDecoderRef.current) videoDecoderRef.current.close();
    if (audioDecoderRef.current) audioDecoderRef.current.close();
    if (renderLoopIdRef.current) cancelAnimationFrame(renderLoopIdRef.current);

    // Kh·ªüi t·∫°o Generator cho Video (Writer)
    const videoTrackGenerator = new MediaStreamTrackGenerator({ kind: 'video' });
    videoWriterRef.current = videoTrackGenerator.writable.getWriter();

    // Kh·ªüi t·∫°o Audio & Video Stream
    initAudioContext();
    const audioTrack = mediaDestinationRef.current.stream.getAudioTracks()[0];
    const combinedStream = new MediaStream([videoTrackGenerator, audioTrack]);

    // G√°n Stream v√†o th·∫ª Video
    if (remoteVideoRef.current) {
      remoteVideoRef.current.srcObject = combinedStream;
      remoteVideoRef.current.muted = false; // Muted false v√¨ √¢m thanh ƒëi qua AudioContext -> Destination
    }

    // Reset tr·∫°ng th√°i
    syncStateRef.current = { firstAudioTimestamp: null, audioContextStartTime: null, isReady: false };
    videoBufferRef.current = [];
    isWaitingForKeyFrame.current = true;

    // B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p v·∫Ω
    startRenderLoop();

    // Setup Video Decoder
    videoDecoderRef.current = new VideoDecoder({
      output: frame => {
        const timestampMs = Math.floor(frame.timestamp / 1000);
        console.log('üé¨ Video Frame Decoded:', {
          timestamp: `${timestampMs}ms`,
          originalTimestamp: `${frame.timestamp}¬µs`,
        });

        // KH√îNG V·∫º NGAY -> ƒê·∫©y v√†o Buffer
        videoBufferRef.current.push(frame);

        // Gi·ªõi h·∫°n Buffer ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ (n·∫øu Audio b·ªã d·ª´ng/l·ªói)
        if (videoBufferRef.current.length > 60) {
          const oldFrame = videoBufferRef.current.shift();
          oldFrame.close();
          console.warn('‚ö†Ô∏è Video Buffer Full: Dropping oldest frame');
        }
      },
      error: e => {
        console.error('VideoDecoder error:', e);
        isWaitingForKeyFrame.current = true;
      },
    });

    // Setup Audio Decoder
    audioDecoderRef.current = new AudioDecoder({
      output: audioData => {
        const timestampMs = Math.floor(audioData.timestamp / 1000);
        console.log('üéµ Audio Frame Decoded:', {
          timestamp: `${timestampMs}ms`,
          originalTimestamp: `${audioData.timestamp}¬µs`,
        });

        playDecodedAudio(audioData, timestampMs);
      },
      error: e => console.error('AudioDecoder error:', e),
    });
  }, [remoteVideoRef, initAudioContext, playDecodedAudio, startRenderLoop]);

  // --- 5. RECEIVE LOOP (MAIN LOGIC) ---
  const mediaDecoder = useCallback(async () => {
    initDecoders();

    while (true) {
      try {
        const data = await nodeRef.current.asyncRecv();
        const buffer = data.buffer.slice(data.byteOffset, data.byteOffset + data.byteLength);

        if (buffer.byteLength < 5) {
          continue;
        }

        const view = new DataView(buffer);
        const frameType = view.getUint8(0);
        const timestamp = view.getUint32(1, false); // Timestamp g·ªëc (ms)

        let payload = frameType === 0 ? buffer.slice(1) : buffer.slice(5);

        // Type 0: CONFIG
        if (frameType === 0) {
          try {
            const configMsg = JSON.parse(new TextDecoder().decode(payload));
            console.log('üìã Config:', configMsg);
            const { videoConfig, audioConfig } = configMsg;

            if (videoConfig && videoDecoderRef.current.state !== 'closed') {
              isWaitingForKeyFrame.current = true;
              const description = Uint8Array.from(atob(videoConfig.description), c => c.charCodeAt(0)).buffer;
              videoDecoderRef.current.configure({ ...videoConfig, description });
            }
            if (audioConfig && audioDecoderRef.current.state !== 'closed') {
              // const description = Uint8Array.from(atob(audioConfig.description), c => c.charCodeAt(0)).buffer;
              audioDecoderRef.current.configure({
                codec: audioConfig.codec,
                sampleRate: audioConfig.sampleRate,
                numberOfChannels: audioConfig.numberOfChannels,
              });
            }
          } catch (e) {
            console.error('Config Error', e);
          }
          continue;
        }

        // Type 1 (Key), Type 2 (Delta): VIDEO
        if (frameType === 1 || frameType === 2) {
          if (videoDecoderRef.current?.state === 'configured') {
            const isKeyFrame = frameType === 1;

            if (isWaitingForKeyFrame.current) {
              if (!isKeyFrame) continue; // B·ªè qua delta frame khi ƒëang ƒë·ª£i key
              isWaitingForKeyFrame.current = false;
              console.log('üéØ Key frame received');
            }

            try {
              const chunk = new EncodedVideoChunk({
                type: isKeyFrame ? 'key' : 'delta',
                timestamp: timestamp * 1000, // ƒê·ªïi sang microseconds cho Decoder
                data: payload,
              });
              videoDecoderRef.current.decode(chunk);
            } catch (e) {
              if (videoDecoderRef.current.state !== 'closed') {
                isWaitingForKeyFrame.current = true; // Reset n·∫øu l·ªói decode
              }
            }
          }
        }

        // Type 3: AUDIO
        else if (frameType === 3) {
          if (audioDecoderRef.current?.state === 'configured') {
            try {
              const chunk = new EncodedAudioChunk({
                type: 'key',
                timestamp: timestamp * 1000,
                data: payload,
              });
              audioDecoderRef.current.decode(chunk);
            } catch (e) {
              // Error handling (re-init logic if needed)
            }
          }
        }
      } catch (error) {
        console.error('Stream Loop Error/End:', error);
        break;
      }
    }
  }, [initDecoders, nodeRef]);

  // Cleanup Final
  useEffect(() => {
    return () => {
      if (renderLoopIdRef.current) cancelAnimationFrame(renderLoopIdRef.current);
      if (videoDecoderRef.current) videoDecoderRef.current.close();
      if (audioDecoderRef.current) audioDecoderRef.current.close();
      if (audioContextRef.current) audioContextRef.current.close();
    };
  }, []);

  return { mediaDecoder };
};
