import { useCallback, useEffect, useRef } from 'react';
import { useLibAV } from './useLibAV';

export const useMediaConsumer = (channelId, videoRef) => {
  const { LibAVWebCodecs } = useLibAV();
  const videoDecoderRef = useRef(null);
  const videoWriterRef = useRef(null);
  const audioDecoderRef = useRef(null);
  const combinedStreamRef = useRef(null);
  const isWaitingForKeyFrame = useRef(true);
  const audioContextRef = useRef(null);
  const mediaDestinationRef = useRef(null);
  const nextStartTimeRef = useRef(0);
  const firstPacketRef = useRef(true);

  const initAudioContext = useCallback(async () => {
    if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });

      mediaDestinationRef.current = audioContextRef.current.createMediaStreamDestination();

      // Resume AudioContext n·∫øu b·ªã suspended (c·∫ßn user interaction)
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }

      nextStartTimeRef.current = audioContextRef.current.currentTime;
      // firstPacketRef.current = true;
    }
  }, []);

  const playDecodedAudio = useCallback(async audioData => {
    try {
      // L·∫•y th√¥ng tin c∆° b·∫£n t·ª´ audioData
      const { numberOfChannels, numberOfFrames, sampleRate, _data } = audioData;
      const audioContext = audioContextRef.current;
      if (!audioContext) return;

      // N·∫øu l√† packet ƒë·∫ßu ti√™n th√¨ b·ªè qua 80 frame ƒë·∫ßu (tr√°nh ti·∫øng pop/noise)
      // const PRE_SKIP = firstPacketRef.current ? 80 : 0;
      // firstPacketRef.current = false;

      // T√≠nh s·ªë frame th·ª±c s·ª± s·∫Ω ph√°t (ƒë√£ b·ªè qua PRE_SKIP)
      // const framesToPlay = numberOfFrames - PRE_SKIP;
      const framesToPlay = numberOfFrames;
      // T·∫°o AudioBuffer v·ªõi s·ªë k√™nh, s·ªë frame v√† sample rate t∆∞∆°ng ·ª©ng
      const audioBuffer = audioContext.createBuffer(numberOfChannels, framesToPlay, sampleRate);

      // Chuy·ªÉn d·ªØ li·ªáu t·ª´ d·∫°ng interleaved (xen k·∫Ω c√°c k√™nh) sang planar (m·ªói k√™nh 1 m·∫£ng ri√™ng)
      // V√≠ d·ª•: [L0, R0, L1, R1, ...] -> k√™nh 0: [L0, L1, ...], k√™nh 1: [R0, R1, ...]
      for (let ch = 0; ch < numberOfChannels; ch++) {
        const channelData = audioBuffer.getChannelData(ch);
        // for (let i = PRE_SKIP; i < numberOfFrames; i++) {
        //   channelData[i - PRE_SKIP] = _data[i * numberOfChannels + ch];
        // }

        for (let i = 0; i < numberOfFrames; i++) {
          channelData[i] = _data[i * numberOfChannels + ch];
        }
      }

      // T·∫°o ngu·ªìn ph√°t √¢m thanh t·ª´ AudioBuffer
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;

      // T·∫°o gainNode ƒë·ªÉ ƒëi·ªÅu ch·ªânh √¢m l∆∞·ª£ng (·ªü ƒë√¢y ƒë·ªÉ m·∫∑c ƒë·ªãnh 1.0)
      const gainNode = audioContext.createGain();
      gainNode.gain.value = 1.0;
      // K·∫øt n·ªëi ngu·ªìn ph√°t qua gainNode t·ªõi MediaStreamDestination (ƒë·ªÉ ph√°t ra loa ho·∫∑c stream)
      source.connect(gainNode).connect(mediaDestinationRef.current);

      // ƒê·∫£m b·∫£o c√°c ƒëo·∫°n √¢m thanh ph√°t n·ªëi ti·∫øp nhau, kh√¥ng b·ªã ch·ªìng ti·∫øng
      if (nextStartTimeRef.current < audioContext.currentTime) nextStartTimeRef.current = audioContext.currentTime;

      // Ph√°t ƒëo·∫°n √¢m thanh t·∫°i th·ªùi ƒëi·ªÉm ƒë√£ t√≠nh to√°n
      source.start(nextStartTimeRef.current);
      // C·∫≠p nh·∫≠t th·ªùi ƒëi·ªÉm ph√°t cho ƒëo·∫°n ti·∫øp theo
      nextStartTimeRef.current += framesToPlay / sampleRate;

      // Gi·∫£i ph√≥ng b·ªô nh·ªõ cho audioData n·∫øu c√≥
      if (audioData && typeof audioData.close === 'function') {
        audioData.close();
      }
    } catch (err) {
      console.error('Error in playDecodedAudio:', err);
      // ƒê·∫£m b·∫£o lu√¥n gi·∫£i ph√≥ng audioData k·ªÉ c·∫£ khi l·ªói
      if (audioData && typeof audioData.close === 'function') {
        audioData.close();
      }
    }
  }, []);

  const initDecoders = useCallback(() => {
    // ƒê√≥ng decoders c≈© tr∆∞·ªõc
    if (videoDecoderRef.current && videoDecoderRef.current.state !== 'closed') {
      videoDecoderRef.current.close();
    }
    if (audioDecoderRef.current && audioDecoderRef.current.state !== 'closed') {
      audioDecoderRef.current.close();
    }

    // T·∫°o video track
    const videoTrackGenerator = new MediaStreamTrackGenerator({ kind: 'video' });
    videoWriterRef.current = videoTrackGenerator.writable.getWriter();

    // T·∫°o AudioContext + MediaStreamDestination
    initAudioContext();

    // L·∫•y audio track t·ª´ AudioContext
    const audioTrack = mediaDestinationRef.current.stream.getAudioTracks()[0];

    const combinedStream = new MediaStream([videoTrackGenerator, audioTrack]);
    combinedStreamRef.current = combinedStream;

    // G√°n cho videoRef
    if (videoRef.current) {
      videoRef.current.srcObject = combinedStream;
      videoRef.current.muted = false;
    }

    isWaitingForKeyFrame.current = true;

    // Init video decoder
    const videoDecoder = new VideoDecoder({
      output: async frame => {
        try {
          await videoWriterRef.current.write(frame);
        } catch (err) {
          frame.close();
        }
      },
      error: err => {
        console.error('VideoDecoder error:', err);
        isWaitingForKeyFrame.current = true;
      },
    });

    videoDecoderRef.current = videoDecoder;

    // Init audio decoder
    const audioDecoder = new LibAVWebCodecs.AudioDecoder({
      output: async audioData => {
        // console.log('--audioData--', audioData);
        playDecodedAudio(audioData);
      },
      error: err => {
        console.error('AudioDecoder error:', err);
      },
    });
    audioDecoderRef.current = audioDecoder;
  }, [videoRef]);

  const connectConsumer = useCallback(() => {
    const ws = new WebSocket(`wss://4044.bandia.vn/consume/${channelId}`);
    ws.binaryType = 'arraybuffer';

    ws.onopen = () => {
      console.log('‚úÖ Connected to consumer WebSocket');
      initDecoders();
    };

    ws.onmessage = event => {
      // üß† 1Ô∏è‚É£ N·∫øu l√† JSON string ‚Üí x·ª≠ l√Ω c·∫•u h√¨nh Decoder
      if (typeof event.data === 'string') {
        try {
          const msg = JSON.parse(event.data);
          if (msg.type === 'DecoderConfigs') {
            if (msg.videoConfig) {
              isWaitingForKeyFrame.current = true;

              console.log('üé• Received Video DecoderConfigs:', msg.videoConfig);
              const videoDescriptionBytes = Uint8Array.from(atob(msg.videoConfig.description), c =>
                c.charCodeAt(0),
              ).buffer;

              videoDecoderRef.current.configure({
                codec: msg.videoConfig.codec,
                codedWidth: msg.videoConfig.codedWidth,
                codedHeight: msg.videoConfig.codedHeight,
                description: videoDescriptionBytes,
              });
            }

            if (msg.audioConfig) {
              console.log('üîä Received Audio DecoderConfigs:', msg.audioConfig);
              const audioDescriptionBytes = Uint8Array.from(atob(msg.audioConfig.description), c =>
                c.charCodeAt(0),
              ).buffer;

              audioDecoderRef.current.configure({
                codec: msg.audioConfig.codec,
                sampleRate: msg.audioConfig.sampleRate,
                numberOfChannels: msg.audioConfig.numberOfChannels,
                // description: audioDescriptionBytes,
              });
            }
          }
        } catch (err) {
          console.warn('‚ö†Ô∏è Invalid JSON message:', err);
        }
        return;
      }

      // üß† 2Ô∏è‚É£ N·∫øu l√† binary ‚Üí x·ª≠ l√Ω g√≥i video
      if (event.data instanceof ArrayBuffer) {
        const buffer = event.data;
        if (buffer.byteLength < 5) {
          console.warn('‚ö†Ô∏è Invalid packet: too small', buffer.byteLength);
          return;
        }

        const view = new DataView(buffer);
        const timestamp = view.getUint32(0, false);
        const frameType = view.getUint8(4);

        const payload = buffer.slice(5);

        // üé• Video frames (key=0, delta=1)
        if (frameType === 0 || frameType === 1) {
          if (!videoDecoderRef.current || videoDecoderRef.current.state !== 'configured') {
            console.warn('‚ö†Ô∏è VideoDecoder not ready, state:', videoDecoderRef.current?.state);
            return;
          }

          const isKeyFrame = frameType === 0;

          // ‚úÖ Ch·ªâ decode khi ƒë√£ c√≥ key frame ho·∫∑c frame hi·ªán t·∫°i l√† key frame
          if (isWaitingForKeyFrame.current) {
            if (!isKeyFrame) {
              console.log('üîÑ Skipping delta frame, still waiting for key frame');
              return;
            }
            console.log('üéØ First key frame received after configure');
          }

          try {
            const chunk = new EncodedVideoChunk({
              type: frameType === 0 ? 'key' : 'delta',
              timestamp: timestamp * 1000,
              data: payload,
            });

            videoDecoderRef.current.decode(chunk);

            // ‚úÖ N·∫øu decode key frame th√†nh c√¥ng, cho ph√©p decode delta frames
            if (isKeyFrame) {
              isWaitingForKeyFrame.current = false;
              console.log('‚úÖ Key frame decoded successfully, now accepting delta frames');
            }
          } catch (e) {
            console.error('‚ùå Video decode error:', e);

            if (e.name === 'InvalidStateError' || e.name === 'DataError') {
              if (videoDecoderRef.current && videoDecoderRef.current.state !== 'closed') {
                videoDecoderRef.current.close();
              }
              videoDecoderRef.current = null;
              isWaitingForKeyFrame.current = true;
            }
          }
        }
        // üîä Audio frames (audio=2)
        else if (frameType === 2) {
          if (!audioDecoderRef.current || audioDecoderRef.current.state !== 'configured') {
            console.warn('‚ö†Ô∏è AudioDecoder not ready, state:', audioDecoderRef.current?.state);
            return;
          }
          try {
            const chunk = new LibAVWebCodecs.EncodedAudioChunk({
              type: 'key',
              timestamp: timestamp * 1000,
              data: payload,
            });
            audioDecoderRef.current.decode(chunk);
          } catch (e) {
            console.error('‚ùå Audio decode error:', e);

            if (e.name === 'InvalidStateError' || e.name === 'DataError') {
              console.log('üîÑ Reinitializing audio decoder due to error...');

              // ƒê√≥ng decoder hi·ªán t·∫°i
              if (audioDecoderRef.current && audioDecoderRef.current.state !== 'closed') {
                audioDecoderRef.current.close();
              }
              audioDecoderRef.current = null;
            }
          }
        } else {
          console.warn('‚ö†Ô∏è Unknown frame type:', frameType);
        }
      } else {
        console.warn('‚ö†Ô∏è Unknown message type:', typeof event.data);
      }
    };

    ws.onerror = err => console.error('‚ùå WS Error:', err);
    ws.onclose = () => {
      console.log('üîå WebSocket closed');
      if (videoDecoderRef.current && videoDecoderRef.current.state !== 'closed') {
        videoDecoderRef.current.close();
      }
      if (videoWriterRef.current) {
        videoWriterRef.current.close();
      }
    };
  }, [channelId, initDecoders]);

  return { connectConsumer };
};
